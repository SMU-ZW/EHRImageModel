{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01ddb6-7c16-4892-8d9e-070661156fc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models, transforms\n",
    "from transformers import ViTConfig, ViTFeatureExtractor, ViTModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6604b7-1600-435f-8fa2-72bbcfd0d6ee",
   "metadata": {},
   "source": [
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92dd6b5-c7d9-496f-8481-85e85ab75edf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "structured_path = Path('all_structured_data.csv')\n",
    "unstructured_path = Path('all_unstructured_data.csv')\n",
    "hdf5_path = Path(\"processed_images_readmission.h5\")\n",
    "pretrained_model_path = Path(\"./chexpert_models/hybrid_encoder_chexpert_best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c9ffd5-3a48-4d85-90ad-5c277b5239b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_processed_images_as_list(hdf5_path):\n",
    "    processed_images = []\n",
    "    dates_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    with h5py.File(hdf5_path, 'r') as hf:\n",
    "        subject_ids = list(hf.keys())  \n",
    "        for subject_id in tqdm(subject_ids, desc=\"Loading HDF5 Samples\"):\n",
    "            grp = hf[subject_id]\n",
    "\n",
    "            if \"images\" not in grp or \"icu_readmission_30d\" not in grp:\n",
    "                continue\n",
    "\n",
    "            images = grp[\"images\"][:]  # (3, 224, 224, 3)\n",
    "            dates = []\n",
    "            for i in range(3):\n",
    "                key = f\"studydate_{i}\"\n",
    "                if key in grp:\n",
    "                    dates.append(grp[key][()].decode(\"utf-8\"))\n",
    "                else:\n",
    "                    dates.append(\"Unknown\")\n",
    "            label = int(grp[\"icu_readmission_30d\"][()])\n",
    "\n",
    "            processed_images.append(images)\n",
    "            dates_list.append(dates)\n",
    "            labels_list.append(label)\n",
    "\n",
    "    print(f\"Total samples: {len(processed_images)}\")\n",
    "    return processed_images, dates_list, labels_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bce305-d6ee-4586-a70e-5136ef6c9811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_images, dates_list, labels_list = load_processed_images_as_list(hdf5_path)\n",
    "\n",
    "print(processed_images[0].shape)  \n",
    "print(dates_list[0])           \n",
    "print(labels_list[0])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb011942-d7f8-459b-9744-c114191589e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_final_feature_h5(hdf5_path):\n",
    "    with h5py.File(hdf5_path, \"r\") as hf:\n",
    "        subject_ids = list(hf.keys())\n",
    "        print(f\"Total samples: {len(subject_ids)}\")\n",
    "        \n",
    "        label_counts = {0: 0, 1: 0, -1: 0}\n",
    "        for sid in subject_ids:\n",
    "            if \"mortality_icu\" in hf[sid]:\n",
    "                label = int(hf[sid][\"mortality_icu\"][()])\n",
    "                if label in label_counts:\n",
    "                    label_counts[label] += 1\n",
    "                else:\n",
    "                    label_counts[-1] += 1 \n",
    "            else:\n",
    "                label_counts[-1] += 1 \n",
    "        \n",
    "        print(f\"Label Distributionï¼š\")\n",
    "        print(f\" - Label 0 : {label_counts[0]}\")\n",
    "        print(f\" - Label 1 : {label_counts[1]}\")\n",
    "        print(f\" - Label -1 : {label_counts[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb52ea-0c03-463e-b90a-591f66ae7af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarize_final_feature_h5(hdf5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd9b433-8620-410b-b6f8-a3a2e118baf8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Hybrid3DPatchEmbedding(nn.Module):\n",
    "    def __init__(self, time_steps=3, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.time_steps = time_steps\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.projection = nn.Conv3d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=(1, patch_size, patch_size),\n",
    "            stride=(1, patch_size, patch_size)\n",
    "        )\n",
    "\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
    "\n",
    "    def forward(self, x):  # x: (B, T, C, H, W)\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.permute(0, 2, 1, 3, 4)  # (B, C, T, H, W)\n",
    "        x = self.projection(x)       # (B, D, T, H', W')\n",
    "        D, T_out, H_out, W_out = x.shape[1], x.shape[2], x.shape[3], x.shape[4]\n",
    "        x = x.permute(0, 2, 3, 4, 1).reshape(B, T_out, H_out * W_out, D)  # (B, T, N, D)\n",
    "        return x + self.pos_embedding.unsqueeze(1)  # (B, T, N, D)\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, mlp_dim=3072):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.msa = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.msa(self.ln1(x), self.ln1(x), self.ln1(x))[0]\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, embed_dim=768):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.proj = nn.Conv2d(2048, embed_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):  # x: (B, T, C, H, W)\n",
    "        B, T, C, H, W = x.shape\n",
    "        res_tokens = []\n",
    "        for t in range(T):\n",
    "            xt = x[:, t]  # (B, C, H, W)\n",
    "            ft = self.backbone(xt)  # (B, 2048, H', W')\n",
    "            ft = self.proj(ft)     # (B, D, H', W')\n",
    "            res_tokens.append(ft.flatten(2).transpose(1, 2))  # (B, N, D)\n",
    "        return torch.stack(res_tokens, dim=1)  # (B, T, N, D)\n",
    "\n",
    "class VisionTransformerHybrid(nn.Module):\n",
    "    def __init__(self, time_steps=3, img_size=224, patch_size=16, in_channels=3, embed_dim=768, num_heads=12, num_layers=12, load_pretrained=False, vit_weight_path=None):\n",
    "        super().__init__()\n",
    "        self.time_steps = time_steps\n",
    "        self.patch_embed = Hybrid3DPatchEmbedding(time_steps, img_size, patch_size, in_channels, embed_dim)\n",
    "        self.transformers = nn.ModuleList([\n",
    "            nn.Sequential(*[TransformerEncoder(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "            for _ in range(time_steps)\n",
    "        ])\n",
    "        self.resnet = ResNetFeatureExtractor(embed_dim)\n",
    "\n",
    "        if load_pretrained and vit_weight_path is not None:\n",
    "            state_dict = torch.load(vit_weight_path, map_location='cpu')\n",
    "            self.patch_embed.load_state_dict(state_dict, strict=False)\n",
    "            print(\"âœ… Patch embedding loaded\")\n",
    "\n",
    "    def forward(self, x):  # x: (B, T, C, H, W)\n",
    "        x_vit = self.patch_embed(x)  # (B, T, N, D)\n",
    "        B, T, N, D = x_vit.shape\n",
    "        vit_out = []\n",
    "        for t in range(T):\n",
    "            xt = x_vit[:, t]  # (B, N, D)\n",
    "            vt = self.transformers[t](xt)  # (B, N, D)\n",
    "            vit_out.append(vt)\n",
    "        vit_out = torch.stack(vit_out, dim=1)  # (B, T, N, D)\n",
    "\n",
    "        resnet_out = self.resnet(x)  # (B, T, 49, D)\n",
    "        fused = torch.cat([vit_out, resnet_out], dim=2)  # âœ… (B, T, 245, D)\n",
    "        return fused\n",
    "\n",
    "def extract_vit_features_with_temporal_structure(images, vit_model, batch_size=32, device='cuda'):\n",
    "    all_features = []\n",
    "    total = len(images)\n",
    "\n",
    "    for i in tqdm(range(0, total, batch_size), desc=\"Extracting Hybrid Features\"):\n",
    "        batch = images[i:i+batch_size]\n",
    "        batch = np.stack(batch)  # (B, T, H, W, C)\n",
    "        batch = batch.transpose(0, 1, 4, 2, 3)  # (B, T, C, H, W)\n",
    "        batch_tensor = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            fused_out = vit_model(batch_tensor)  # (B, T, 245, D)\n",
    "            all_features.append(fused_out.cpu())\n",
    "\n",
    "    return torch.cat(all_features, dim=0)  # (N, T, 245, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb4bf3f-268c-4fb1-b63e-f9f8106c7c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_isoformat(d):\n",
    "    if '.' in d:\n",
    "        head, frac = d.split('.')\n",
    "        frac = frac[:6] \n",
    "        return f\"{head}.{frac}\"\n",
    "    return d\n",
    "\n",
    "class TimeBiasLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.a = nn.Parameter(torch.tensor(1.0))  \n",
    "        self.c = nn.Parameter(torch.tensor(0.0))  \n",
    "\n",
    "    def forward(self, R):  # R: (B, T, T)\n",
    "        return 1 / (1 + torch.exp(self.a * R - self.c))  \n",
    "\n",
    "class DistanceAwareAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.time_bias = TimeBiasLayer()\n",
    "\n",
    "    def forward(self, x, R):  # x: (B, T, N, D), R: (B, T, T)\n",
    "        B, T, N, D = x.shape\n",
    "\n",
    "        Q = self.q_proj(x).view(B, T, N, self.num_heads, self.head_dim).permute(0, 3, 1, 2, 4)\n",
    "        K = self.k_proj(x).view(B, T, N, self.num_heads, self.head_dim).permute(0, 3, 1, 2, 4)\n",
    "        V = self.v_proj(x).view(B, T, N, self.num_heads, self.head_dim).permute(0, 3, 1, 2, 4)\n",
    "\n",
    "        Q = Q.reshape(B, self.num_heads, T * N, self.head_dim)\n",
    "        K = K.reshape(B, self.num_heads, T * N, self.head_dim)\n",
    "        V = V.reshape(B, self.num_heads, T * N, self.head_dim)\n",
    "\n",
    "        attn_logits = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_logits = torch.relu(attn_logits)\n",
    "\n",
    "        R_hat = self.time_bias(R)  # (B, T, T)\n",
    "        R_hat = R_hat.unsqueeze(1).repeat(1, self.num_heads, 1, 1)  # (B, H, T, T)\n",
    "        R_hat = R_hat.repeat_interleave(N, dim=2).repeat_interleave(N, dim=3)  # (B, H, T*N, T*N)\n",
    "\n",
    "        attn_weights = attn_logits * R_hat\n",
    "        attn_probs = torch.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        context = torch.matmul(attn_probs, V)\n",
    "        context = context.view(B, self.num_heads, T, N, self.head_dim).permute(0, 2, 3, 1, 4).contiguous()\n",
    "        context = context.view(B, T, N, D)\n",
    "\n",
    "        return self.out_proj(context)\n",
    "\n",
    "class DistanceAwareTemporalTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                nn.LayerNorm(embed_dim),\n",
    "                DistanceAwareAttention(embed_dim, num_heads),\n",
    "                nn.LayerNorm(embed_dim),\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(embed_dim, embed_dim * 4),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(embed_dim * 4, embed_dim)\n",
    "                )\n",
    "            ]) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.final_norm = nn.LayerNorm(embed_dim)\n",
    "        self.output_layer = nn.Linear(embed_dim, embed_dim)  \n",
    "\n",
    "    def forward(self, x, R):  # x: (B, T, N, D), R: (B, T, T)\n",
    "        for ln1, attn, ln2, ff in self.layers:\n",
    "            x = x + attn(ln1(x), R)\n",
    "            x = x + ff(ln2(x))\n",
    "        x = self.final_norm(x)\n",
    "        return self.output_layer(x)  # (B, T, N, D)\n",
    "\n",
    "def build_time_distance_matrix(study_dates_batch):\n",
    "    batch_timestamps = []\n",
    "    for dates in study_dates_batch:\n",
    "        ts = [datetime.fromisoformat(clean_isoformat(d)).timestamp() for d in dates]\n",
    "        Tn = ts[-1]\n",
    "        distances = [[abs(Tn - t_i) for t_i in ts] for _ in ts]\n",
    "        batch_timestamps.append(distances)\n",
    "    return torch.tensor(batch_timestamps, dtype=torch.float32)\n",
    "\n",
    "def run_temporal_encoder_example(fused_features):\n",
    "    model = DistanceAwareTemporalTransformer().to(\"cuda\").eval()\n",
    "    all_outputs = []\n",
    "\n",
    "    for i in range(len(fused_features)):\n",
    "        x = fused_features[i].unsqueeze(0).to(\"cuda\")  # (1, 3, 245, 768)\n",
    "        dates = [dates_list[i]]  # [[t0, t1, t2]]\n",
    "        R = build_time_distance_matrix(dates).to(\"cuda\")  # (1, 3, 3)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(x, R)  # (1, 3, 245, 768)\n",
    "        all_outputs.append(output.squeeze(0).cpu())\n",
    "\n",
    "    return all_outputs  # List of (3, 245, 768)\n",
    "\n",
    "class ReadmissionPredictor(nn.Module):\n",
    "    def __init__(self, embed_dim=768):  \n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x).squeeze(1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961c3b0-f607-4e59-892c-7fddbfe97d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_pretrained_keys(pretrained_dict, model_dict):\n",
    "    new_dict = {}\n",
    "    missing_keys = []  # List to store missing keys\n",
    "    loaded_keys_count = 0  # Counter for valid keys loaded\n",
    "    \n",
    "    for k, v in pretrained_dict.items():\n",
    "        new_key = k\n",
    "        if new_key not in model_dict:\n",
    "            print(f\"Warning: Key '{k}' from the pretrained model does not have a direct match in the model.\")\n",
    "            missing_keys.append(k)  # Track the missing key\n",
    "            continue\n",
    "        new_dict[new_key] = v  # If matched, add to the new dictionary\n",
    "        loaded_keys_count += 1  # Increment counter for valid keys loaded\n",
    "        \n",
    "    return new_dict, missing_keys, loaded_keys_count\n",
    "\n",
    "def map_pretrained_to_current(pretrained_keys, current_keys):\n",
    "    mapping = {}\n",
    "\n",
    "    for p_key in pretrained_keys:\n",
    "        for c_key in current_keys:\n",
    "            if p_key.replace('backbone.', '') == c_key.replace('backbone.', ''):\n",
    "                mapping[p_key] = c_key\n",
    "                break\n",
    "\n",
    "    return mapping\n",
    "\n",
    "pretrained_keys = [\n",
    "    'backbone.patch_embed.pos_embedding', 'backbone.patch_embed.projection.weight',\n",
    "    'backbone.patch_embed.projection.bias', 'backbone.transformers.0.0.ln1.weight', 'backbone.transformers.0.0.ln1.bias',\n",
    "    'backbone.transformers.0.0.msa.in_proj_weight', 'backbone.transformers.0.0.msa.in_proj_bias',\n",
    "    'backbone.transformers.0.0.msa.out_proj.weight', 'backbone.transformers.0.0.msa.out_proj.bias',\n",
    "    'backbone.transformers.0.0.ln2.weight', 'backbone.transformers.0.0.ln2.bias', 'backbone.transformers.0.0.mlp.0.weight',\n",
    "    'backbone.transformers.0.0.mlp.0.bias', 'backbone.transformers.0.0.mlp.2.weight', 'backbone.transformers.0.0.mlp.2.bias',\n",
    "    'backbone.transformers.0.1.ln1.weight', 'backbone.transformers.0.1.ln1.bias', 'backbone.transformers.0.1.msa.in_proj_weight',\n",
    "    'backbone.transformers.0.1.msa.in_proj_bias', 'backbone.transformers.0.1.msa.out_proj.weight',\n",
    "    'backbone.transformers.0.1.msa.out_proj.bias', 'backbone.transformers.0.1.ln2.weight', 'backbone.transformers.0.1.ln2.bias',\n",
    "    'backbone.transformers.0.1.mlp.0.weight', 'backbone.transformers.0.1.mlp.0.bias', 'backbone.transformers.0.1.mlp.2.weight',\n",
    "    'backbone.transformers.0.1.mlp.2.bias', 'backbone.resnet.backbone.0.weight', 'backbone.resnet.backbone.1.weight',\n",
    "    'backbone.resnet.backbone.1.bias', 'backbone.resnet.backbone.1.running_mean', 'backbone.resnet.backbone.1.running_var'\n",
    "]\n",
    "\n",
    "current_keys = [\n",
    "    'patch_embed.pos_embedding', 'patch_embed.projection.weight', 'patch_embed.projection.bias',\n",
    "    'transformers.0.0.ln1.weight', 'transformers.0.0.ln1.bias', 'transformers.0.0.msa.in_proj_weight',\n",
    "    'transformers.0.0.msa.in_proj_bias', 'transformers.0.0.msa.out_proj.weight', 'transformers.0.0.msa.out_proj.bias',\n",
    "    'transformers.0.0.ln2.weight', 'transformers.0.0.ln2.bias', 'transformers.0.0.mlp.0.weight', 'transformers.0.0.mlp.0.bias',\n",
    "    'transformers.0.0.mlp.2.weight', 'transformers.0.0.mlp.2.bias', 'transformers.0.1.ln1.weight', 'transformers.0.1.ln1.bias',\n",
    "    'transformers.0.1.msa.in_proj_weight', 'transformers.0.1.msa.in_proj_bias', 'transformers.0.1.msa.out_proj.weight',\n",
    "    'transformers.0.1.msa.out_proj.bias', 'transformers.0.1.ln2.weight', 'transformers.0.1.ln2.bias', 'transformers.0.1.mlp.0.weight',\n",
    "    'transformers.0.1.mlp.0.bias', 'transformers.0.1.mlp.2.weight', 'transformers.0.1.mlp.2.bias', 'resnet.backbone.0.weight',\n",
    "    'resnet.backbone.1.weight', 'resnet.backbone.1.bias', 'resnet.backbone.1.running_mean', 'resnet.backbone.1.running_var'\n",
    "]\n",
    "\n",
    "mapping_result = map_pretrained_to_current(pretrained_keys, current_keys)\n",
    "\n",
    "for p_key, c_key in mapping_result.items():\n",
    "    print(f\"Pretrained Key: {p_key} -> Current Key: {c_key}\")\n",
    "\n",
    "\n",
    "def load_pretrained_model(model, pretrained_model_path, device='cuda'):\n",
    "    state_dict = torch.load(pretrained_model_path, map_location=device)\n",
    "    \n",
    "    state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    \n",
    "    pretrained_keys = list(state_dict.keys())\n",
    "    current_keys = list(model.state_dict().keys())\n",
    "\n",
    "    mapping_result = map_pretrained_to_current(pretrained_keys, current_keys)\n",
    "\n",
    "    new_state_dict = {}\n",
    "    missing_keys = []\n",
    "    loaded_keys_count = 0\n",
    "\n",
    "    for p_key, c_key in mapping_result.items():\n",
    "        if c_key in model.state_dict():\n",
    "            new_state_dict[c_key] = state_dict[p_key]  \n",
    "            loaded_keys_count += 1\n",
    "        else:\n",
    "            missing_keys.append(p_key)  \n",
    "\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "\n",
    "    print(f\"âœ… Pre-trained model loaded from {pretrained_model_path}\")\n",
    "    if missing_keys:\n",
    "        print(f\"âŒ Missing keys: {missing_keys}\")\n",
    "    else:\n",
    "        print(\"All keys matched successfully!\")\n",
    "    print(f\"ðŸ”‘ {loaded_keys_count} valid keys were loaded from the pretrained model.\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vit_model = VisionTransformerHybrid().to(device)\n",
    "\n",
    "vit_model = load_pretrained_model(vit_model, pretrained_model_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2abc6d-f701-4564-92dd-ff3145de5d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vit_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662003c3-5892-46ca-ab0d-383334a86961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63dc2d-14a4-482b-925f-e2fef58196d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_temporal_features(\n",
    "    processed_images, dates_list, labels_list,\n",
    "    epochs=300, batch_size=32, lr=1e-4, device='cpu',\n",
    "    save_path='model_checkpoint.pth',\n",
    "    enable_temporal_grad=False,\n",
    "    enable_vit_grad=False,\n",
    "    use_pretrained_vit=True,\n",
    "    pretrained_model_path='pretrained.pth'\n",
    "):\n",
    "    vit_model = VisionTransformerHybrid().to(device)\n",
    "    if use_pretrained_vit:\n",
    "        vit_model = load_pretrained_model(vit_model, pretrained_model_path, device)\n",
    "    vit_model.train(mode=enable_vit_grad)\n",
    "    for param in vit_model.parameters():\n",
    "        param.requires_grad = enable_vit_grad\n",
    "\n",
    "    all_fused_features = []\n",
    "    for i in tqdm(range(0, len(processed_images), batch_size), desc=\"Extracting Hybrid Features\"):\n",
    "        batch_images = processed_images[i:i+batch_size]\n",
    "        batch = np.stack(batch_images)\n",
    "        batch = batch.transpose(0, 1, 4, 2, 3)\n",
    "        batch_tensor = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(enable_vit_grad):\n",
    "            fused_out = checkpoint(vit_model, batch_tensor)\n",
    "            all_fused_features.append(fused_out.cpu())\n",
    "\n",
    "        del batch_tensor, batch\n",
    "        gc.collect()\n",
    "\n",
    "    fused_features = torch.cat(all_fused_features, dim=0)\n",
    "\n",
    "    temporal_model = DistanceAwareTemporalTransformer().to(device)\n",
    "    temporal_model.train(mode=enable_temporal_grad)\n",
    "    for param in temporal_model.parameters():\n",
    "        param.requires_grad = enable_temporal_grad\n",
    "\n",
    "    all_temporal_outputs = []\n",
    "    for i in tqdm(range(len(fused_features)), desc=\"Temporal Modeling\"):\n",
    "        x = fused_features[i].unsqueeze(0).to(device)\n",
    "        dates = [dates_list[i]]\n",
    "        R = build_time_distance_matrix(dates).to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(enable_temporal_grad):\n",
    "            temporal_output = temporal_model(x, R)\n",
    "            all_temporal_outputs.append(temporal_output.squeeze(0).cpu())\n",
    "\n",
    "        del x, R, temporal_output\n",
    "        gc.collect()\n",
    "\n",
    "    temporal_outputs = torch.stack(all_temporal_outputs, dim=0)\n",
    "\n",
    "    pooled = [x.mean(dim=(0, 1)) for x in temporal_outputs]\n",
    "    X = torch.stack(pooled)\n",
    "    y = torch.tensor(labels_list).float()\n",
    "\n",
    "    ros = RandomOverSampler(sampling_strategy='auto', random_state=42)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X.numpy(), y.numpy())\n",
    "\n",
    "    resampled_train_dataset = TensorDataset(\n",
    "        torch.tensor(X_resampled, dtype=torch.float32),\n",
    "        torch.tensor(y_resampled, dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "    val_set = TensorDataset(X, y)\n",
    "    test_set = TensorDataset(X, y)\n",
    "\n",
    "    train_loader = DataLoader(resampled_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "    classifier = ReadmissionPredictor(embed_dim=768).to(device)\n",
    "    optimizer = torch.optim.AdamW(classifier.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_auc = 0\n",
    "    for epoch in range(epochs):\n",
    "        classifier.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, (X_batch, y_batch) in enumerate(train_loader):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            logits = classifier(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            if (step + 1) % 4 == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "        classifier.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                logits = classifier(X_batch).sigmoid().cpu()\n",
    "                all_preds.extend(logits.numpy())\n",
    "                all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "        preds_bin = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        auc_score = roc_auc_score(all_labels, all_preds)\n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, AUC={auc_score:.4f}, F1={f1_score(all_labels, preds_bin):.4f}\")\n",
    "        \n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'classifier_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'auc': auc_score\n",
    "            }, os.path.join(save_path, f\"model_epoch_{epoch+1}.pth\"))\n",
    "\n",
    "        if auc_score > best_auc:\n",
    "            best_auc = auc_score\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'classifier_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'auc': best_auc\n",
    "            }, os.path.join(save_path, f\"model_best.pth\"))\n",
    "\n",
    "    print(\"\\nBest model saved to:\", save_path)\n",
    "\n",
    "    classifier.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            logits = classifier(X_batch).sigmoid().cpu()\n",
    "            all_preds.extend(logits.numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    preds_bin = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    print(\"\\nTest Results:\")\n",
    "    print(f\"AUC       : {roc_auc_score(all_labels, all_preds):.4f}\")\n",
    "    print(f\"Accuracy  : {accuracy_score(all_labels, preds_bin):.4f}\")\n",
    "    print(f\"F1 Score  : {f1_score(all_labels, preds_bin):.4f}\")\n",
    "    print(f\"Precision : {precision_score(all_labels, preds_bin):.4f}\")\n",
    "    print(f\"Recall    : {recall_score(all_labels, preds_bin):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98175909-e9fc-4d96-bc27-1d0319369aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"./cxr_models/pretrained_readmission_{datetime.now().strftime('%Y%m%d_%H%M%S')}/\"\n",
    "train_model_with_temporal_features(\n",
    "    processed_images, dates_list, labels_list,\n",
    "    epochs=1000, batch_size=32, lr=1e-4, device='cpu',\n",
    "    save_path=save_path,\n",
    "    enable_temporal_grad=False,\n",
    "    enable_vit_grad=False,\n",
    "    use_pretrained_vit=True,\n",
    "    pretrained_model_path=pretrained_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a4c8b-8c73-47d1-8c71-436d65dedef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"./cxr_models/readmission_{datetime.now().strftime('%Y%m%d_%H%M%S')}/\"\n",
    "train_model_with_temporal_features(\n",
    "    processed_images, dates_list, labels_list,\n",
    "    epochs=1000, batch_size=32, lr=1e-4, device='cpu',\n",
    "    save_path=save_path,\n",
    "    enable_temporal_grad=False,\n",
    "    enable_vit_grad=False,\n",
    "    use_pretrained_vit=False,\n",
    "    pretrained_model_path=pretrained_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfd147d-fd8c-4525-a707-ecbdff90e992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
